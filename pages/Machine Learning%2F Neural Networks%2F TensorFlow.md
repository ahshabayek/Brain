- coffee roasting
	- ![image.png](../assets/image_1675166754395_0.png)
	- undercooked
	- overcooked
-
- Dense (fully connected)
	- ![image.png](../assets/image_1675166827905_0.png)
	- ![image.png](../assets/image_1675199160724_0.png)
	-
- Tensorflow data representation:
	- feature vectors:
		- numpy version:
		- ![image.png](../assets/image_1675200725675_0.png)
		- ![image.png](../assets/image_1675200803318_0.png)
	- Tensorflow only uses matrices nt vectors.
	- ![image.png](../assets/image_1675200919551_0.png)
	- tensor is the type used for tensor flow and needs conversion back and forth between numpy.
	- ![image.png](../assets/image_1675284781477_0.png)
	- sequential function patches layers to create a nn.
	- ![image.png](../assets/image_1675284910836_0.png)
	- ![image.png](../assets/image_1675284980536_0.png)
- Numpy naive implementation
	- ![image.png](../assets/image_1675290985482_0.png)
	- ![image.png](../assets/image_1675377901678_0.png)
- Vectorization of neural networks.
	- ![image.png](../assets/image_1675379478144_0.png)
	- ![image.png](../assets/image_1675537013168_0.png)
	- ![image.png](../assets/image_1675555446609_0.png)
	-
	- ![image.png](../assets/image_1675555390965_0.png)
		- Logistic loss for classification  or logistic regression, while mean loss is for regression problems(mean squared error)
	- [[Machine Learning/Logistic Loss]]
	- Steps:
		- define model and activation function.
		- define loss and cost function.
			- ![image.png](../assets/image_1675555868118_0.png)
		- gradient descent or correction function.
			- ![image.png](../assets/image_1675555695721_0.png)
- Activations functions:
	- ![image.png](../assets/image_1675557441411_0.png)
	- Most popular:
		- ![image.png](../assets/image_1675557526661_0.png)
	- How to choose for output layer:
		- binary classification sigmoid is often best
		- regression linear activation is better: values can be negative aswell
		- ![image.png](../assets/image_1675557716550_0.png)
	- How to chose hidden layers:
		- why use relu instead of sigmoid:
			- sig. less efficient in computing
			- 2 flat regions. flatness slows down gradient decent...why?flattens scoring potential
			- ![image.png](../assets/image_1675597677339_0.png)
			-
	- why use activation functions
		- ![image.png](../assets/image_1675598175240_0.png)
		- Linear function of a linear function is a linear function therefore
		- ![image.png](../assets/image_1675598245041_0.png)
		- why is RELU so different than a linear unit:
			- ![image.png](../assets/image_1675599075103_0.png)
			- ![image.png](../assets/image_1675599175025_0.png)
			-
-